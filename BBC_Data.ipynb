{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BBC News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Library installation and import. Installation line is commented as it was installed once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install selenium\n",
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Retrieving HTML content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser = webdriver.Firefox()\n",
    "browser.get('https://www.bbc.com/news/world/europe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Extracting articles. \n",
    "It was found that there are few \"hot\" articles displayed at each page, while all the articles including the \"hot\" ones can be retrieved from Latest Updates section. For that reason function extract_articles() below concentrates on that section. Selectors were identified manually using inspect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'headline': 'The long, slow road to a ceasefire, with no guarantee of success', 'summary': 'The agreement comes after days of talks with the US in Saudi Arabia.', 'link': 'https://www.bbc.com/news/articles/cqjdnpjgj75o', 'time': '11 hrs ago'}\n",
      "{'headline': 'Two French air display jets crash in rehearsal', 'summary': 'The three occupants of the two Alpha Jets that collided ejected and were \"found alive and conscious\".', 'link': 'https://www.bbc.com/news/videos/cpv4gxx2dwzo', 'time': '14 hrs ago'}\n",
      "{'headline': 'Dáil adjourns as rowdy scenes erupt over speaking rights', 'summary': 'Rowdy scenes erupted while the Irish PM attempted to explain a new government proposal for speaking rights.', 'link': 'https://www.bbc.com/news/articles/cx20qyzgrzxo', 'time': '14 hrs ago'}\n",
      "{'headline': \"Grandparents arrested on suspicion of toddler's murder in French Alps\", 'summary': 'Emile Soleil mysteriously vanished in 2023, in a case that made headlines across France.', 'link': 'https://www.bbc.com/news/articles/cvge6jx3l90o', 'time': '19 hrs ago'}\n",
      "{'headline': 'Ex-Fifa chief Sepp Blatter and Michel Platini cleared of corruption', 'summary': \"Tuesday's ruling is the second time the two men have been acquitted over payment made in 2011.\", 'link': 'https://www.bbc.com/news/articles/cx298rz985eo', 'time': '20 hrs ago'}\n",
      "{'headline': \"'We're trying to bring democracy back': BBC on the ground in Istanbul\", 'summary': \"The BBC's Mark Lowen is on the ground in Istanbul, as the protests in the country continue for a sixth night.\", 'link': 'https://www.bbc.com/news/videos/c2d46327npro', 'time': '1 day ago'}\n",
      "{'headline': 'Ros Atkins on... the media crackdown in Turkey', 'summary': \"The BBC's Analysis Editor looks at the targeting of press freedom by the Turkish government, as protests in the country continue.\", 'link': 'https://www.bbc.com/news/videos/crmjy73jy9jo', 'time': '1 day ago'}\n",
      "{'headline': 'Two women who spied for Russia tracked down and named by BBC', 'summary': 'Bulgarian nationals Cvetelina Gencheva and Tsvetanka Doncheva worked with six spies convicted in London.', 'link': 'https://www.bbc.com/news/articles/cqx0v599wqvo', 'time': '2 days ago'}\n",
      "{'headline': \"Erdogan: Turkey's all-powerful leader of 20 years\", 'summary': 'How Recep Tayyip Erdogan rose from humble beginnings to becoming a political giant.', 'link': 'https://www.bbc.com/news/world-europe-13746679', 'time': '2 days ago'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def extract_articles(driver):\n",
    "    \"\"\"Extracts articles from the current page using an existing Selenium WebDriver.\"\"\"\n",
    "    try:\n",
    "        # Wait for the \"Latest Updates\" section to load\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, \"div[data-testid='alaska-section']\"))\n",
    "        )\n",
    "\n",
    "        # Find all article links inside \"Latest Updates\"\n",
    "        articles = driver.find_elements(By.CSS_SELECTOR, \"div[data-testid='alaska-section'] a[data-testid='internal-link']\")\n",
    "\n",
    "        extracted_articles = []\n",
    "        for article in articles:\n",
    "            try:\n",
    "                headline = article.find_element(By.CSS_SELECTOR, \"h2[data-testid='card-headline']\").text\n",
    "                summary = article.find_element(By.CSS_SELECTOR, \"p[data-testid='card-description']\").text\n",
    "                link = article.get_attribute(\"href\")\n",
    "                time_updated = article.find_element(By.CSS_SELECTOR, \"span[data-testid='card-metadata-lastupdated']\").text\n",
    "\n",
    "                extracted_articles.append({\n",
    "                    \"headline\": headline,\n",
    "                    \"summary\": summary,\n",
    "                    \"link\": link,\n",
    "                    \"time\": time_updated\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting article: {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load articles: {e}\")\n",
    "        extracted_articles = []\n",
    "\n",
    "    return extracted_articles  # Driver stays open for pagination (see subtask 4)\n",
    "\n",
    "#Initialize WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://www.bbc.com/news/world/europe\")  # Load the initial page\n",
    "\n",
    "#Now call extract_articles with the existing driver\n",
    "europe_articles = extract_articles(driver)\n",
    "\n",
    "#Print extracted articles\n",
    "for article in europe_articles:\n",
    "    print(article)\n",
    "\n",
    "#Quit WebDriver after scraping\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Scrape multiple pages.\n",
    "Function extract_pages() given below identifies number of pages and then goes through them using next (\">\") button and calling extract_articles() function defined above to get all the articles. Selectors were identified manually. The function also handles cookies popup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cookies accepted.\n",
      "Total pages detected: 12\n",
      "Total articles scraped: 100\n"
     ]
    }
   ],
   "source": [
    "def extract_pages(section_url):\n",
    "    \"\"\"Scrapes all paginated articles in the 'Latest Updates' section, handling dynamic loading.\"\"\"\n",
    "\n",
    "    driver.get(section_url)\n",
    "    all_articles = []\n",
    "\n",
    "    try:\n",
    "        # Handle Cookie Popup\n",
    "        try:\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.frame_to_be_available_and_switch_to_it((By.ID, \"sp_message_iframe_1192447\"))\n",
    "            )\n",
    "            accept_button = WebDriverWait(driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, \"//button[text()='I agree']\"))\n",
    "            )\n",
    "            accept_button.click()\n",
    "            print(\"Cookies accepted.\")\n",
    "            driver.switch_to.default_content()\n",
    "            time.sleep(2)\n",
    "        except:\n",
    "            print(\"No cookie popup found.\")\n",
    "\n",
    "        # Detect total pages from pagination buttons\n",
    "        pagination_buttons = driver.find_elements(By.CSS_SELECTOR, \"button.RZsRF.gBqyGL\")\n",
    "        total_pages = max([int(btn.text) for btn in pagination_buttons if btn.text.isdigit()], default=1)\n",
    "        print(f\"Total pages detected: {total_pages}\")\n",
    "\n",
    "        scraped_headlines = set()  # Track already scraped headlines\n",
    "\n",
    "        for page in range(1, total_pages + 1):\n",
    "            #print(f\"- Scraping page {page}...\")\n",
    "\n",
    "            # Extract articles before clicking\n",
    "            articles = extract_articles(driver)\n",
    "            new_articles = [a for a in articles if a['headline'] not in scraped_headlines]\n",
    "            #for article in articles[:5]:  # Display only first 5 articles\n",
    "            #     print(article)\n",
    "\n",
    "            #if not new_articles:\n",
    "            #    print(\"No new articles found. Stopping pagination.\")\n",
    "            #    break  # Stop if the page didn't change\n",
    "\n",
    "            # Add only new articles\n",
    "            all_articles.extend(new_articles)\n",
    "            scraped_headlines.update([a['headline'] for a in new_articles])\n",
    "\n",
    "            time.sleep(1)\n",
    "\n",
    "            # Click the \"Next Page\" button\n",
    "            try:\n",
    "                driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                time.sleep(2)\n",
    "                #print(f\"Clicking Next Page button (Page {page})\")\n",
    "                next_button = WebDriverWait(driver, 10).until(\n",
    "                    EC.presence_of_element_located((By.CSS_SELECTOR, \"button[data-testid='pagination-next-button']\"))\n",
    "                )\n",
    "                \n",
    "\n",
    "                driver.execute_script(\"document.querySelector('button[data-testid=\\\"pagination-next-button\\\"]').click();\")\n",
    "\n",
    "                time.sleep(1)  # Wait for new articles to load\n",
    "            except:\n",
    "                print(\"No more pages to scrape.\")\n",
    "                break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "    return all_articles\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Execution for the Europe section\n",
    "europe_articles = extract_pages(\"https://www.bbc.com/news/world/europe\")\n",
    "\n",
    "# Print results\n",
    "print(f\"Total articles scraped: {len(europe_articles)}\")\n",
    "#for article in europe_articles:  \n",
    "    #print(article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Expand the Scope. Here the extract_pages() function defined above is executed for the defined list of regions to get all the required articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Scraping region: US & Canada\n",
      "Cookies accepted.\n",
      "Total pages detected: 12\n",
      " 100 articles scraped from US & Canada\n",
      "-- Scraping region: UK\n",
      "No cookie popup found.\n",
      "Total pages detected: 1\n",
      " 9 articles scraped from UK\n",
      "-- Scraping region: Africa\n",
      "No cookie popup found.\n",
      "Total pages detected: 12\n",
      " 100 articles scraped from Africa\n",
      "-- Scraping region: Asia\n",
      "No cookie popup found.\n",
      "Total pages detected: 12\n",
      " 99 articles scraped from Asia\n",
      "-- Scraping region: Australia\n",
      "No cookie popup found.\n",
      "Total pages detected: 12\n",
      " 100 articles scraped from Australia\n",
      "-- Scraping region: Europe\n",
      "No cookie popup found.\n",
      "Total pages detected: 12\n",
      " 100 articles scraped from Europe\n",
      "-- Scraping region: Latin America\n",
      "No cookie popup found.\n",
      "Total pages detected: 12\n",
      "Error extracting article: Message: no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"p[data-testid='card-description']\"}\n",
      "  (Session info: chrome=134.0.6998.166); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "0   chromedriver                        0x000000010279fb78 cxxbridge1$str$ptr + 2778912\n",
      "1   chromedriver                        0x00000001027981b0 cxxbridge1$str$ptr + 2747736\n",
      "2   chromedriver                        0x00000001022ede24 cxxbridge1$string$len + 92932\n",
      "3   chromedriver                        0x0000000102335158 cxxbridge1$string$len + 384568\n",
      "4   chromedriver                        0x000000010232a800 cxxbridge1$string$len + 341216\n",
      "5   chromedriver                        0x0000000102376500 cxxbridge1$string$len + 651744\n",
      "6   chromedriver                        0x00000001023292e4 cxxbridge1$string$len + 335812\n",
      "7   chromedriver                        0x0000000102765d04 cxxbridge1$str$ptr + 2541740\n",
      "8   chromedriver                        0x0000000102768fc8 cxxbridge1$str$ptr + 2554736\n",
      "9   chromedriver                        0x0000000102746a44 cxxbridge1$str$ptr + 2414060\n",
      "10  chromedriver                        0x0000000102769828 cxxbridge1$str$ptr + 2556880\n",
      "11  chromedriver                        0x0000000102737998 cxxbridge1$str$ptr + 2352448\n",
      "12  chromedriver                        0x00000001027883a4 cxxbridge1$str$ptr + 2682700\n",
      "13  chromedriver                        0x000000010278852c cxxbridge1$str$ptr + 2683092\n",
      "14  chromedriver                        0x0000000102797e24 cxxbridge1$str$ptr + 2746828\n",
      "15  libsystem_pthread.dylib             0x000000019c55d240 _pthread_start + 148\n",
      "16  libsystem_pthread.dylib             0x000000019c558024 thread_start + 8\n",
      "\n",
      " 99 articles scraped from Latin America\n",
      "-- Scraping region: Middle East\n",
      "No cookie popup found.\n",
      "Total pages detected: 12\n",
      " 100 articles scraped from Middle East\n",
      "\n",
      "Total articles scraped from all regions: 707\n",
      "{'headline': \"'It shouldn't happen again' - Americans react to Signal group chat leak\", 'summary': 'The White House has confirmed a journalist was inadvertently added to a chat where national security officials discussed a military strike.', 'link': 'https://www.bbc.com/news/videos/crknyvk5z4no', 'time': '6 hrs ago'}\n",
      "{'headline': 'JD Vance will join wife on Greenland trip', 'summary': \"Greenland's leaders have criticised planned visits by US officials after Trump's threats to annex the island.\", 'link': 'https://www.bbc.com/news/articles/cvgwjllld1ro', 'time': '7 hrs ago'}\n",
      "{'headline': 'Trump signs order aimed at overhauling US elections', 'summary': 'The executive order would require proof of US citizenship on elections forms among other changes.', 'link': 'https://www.bbc.com/news/articles/cdxq37nxl55o', 'time': '7 hrs ago'}\n",
      "{'headline': 'Columbia student protester sues Trump to stop deportation', 'summary': 'The Trump administration has been clamping down on students who have attended pro-Palestinian protests.', 'link': 'https://www.bbc.com/news/articles/c70wprgper4o', 'time': '8 hrs ago'}\n",
      "{'headline': \"India accused of meddling in Canada's Conservative Party race\", 'summary': 'Pierre Poilievre says he won the leadership \"fair and square\" following reports alleging India meddled in the party\\'s 2022 contest.', 'link': 'https://www.bbc.com/news/articles/ckgnl4e3grpo', 'time': '9 hrs ago'}\n",
      "{'headline': 'Watch: How the Signal group chat fallout unfolded over 24 hours', 'summary': 'Some of the key reactions to reports that a journalist was inadvertently added to a chat with high-level Trump officials discussing air strikes in Yemen.', 'link': 'https://www.bbc.com/news/videos/cyver5v6197o', 'time': '16 hrs ago'}\n",
      "{'headline': \"Goldberg says officials got 'lucky' it was him inadvertently added to group chat\", 'summary': \"The Atlantic's Jeffrey Goldberg talks about what he witnessed in the Signal group chat.\", 'link': 'https://www.bbc.com/news/videos/czxnjp0yvwro', 'time': '19 hrs ago'}\n",
      "{'headline': 'Five takeaways from leaked US top military chat group', 'summary': 'What was discussed in the top-secret group that a journalist was accidentally added to?', 'link': 'https://www.bbc.com/news/articles/cr52yrgq48no', 'time': '20 hrs ago'}\n",
      "{'headline': 'Why is it a problem if Yemen strike plans shared on Signal?', 'summary': \"US officials' use of an unsecured chat app presented potential problems even before a journalist was added to the group, analysts say.\", 'link': 'https://www.bbc.com/news/articles/czx7l1q2qdko', 'time': '20 hrs ago'}\n",
      "{'headline': 'Trump bemoans a portrait of him - but gets a new one from Putin', 'summary': 'A \"distorted\" portrait in the US was removed after the president complained, but he was \"touched\" by a Russian gift.', 'link': 'https://www.bbc.com/news/articles/c62xyrr20dxo', 'time': '1 day ago'}\n"
     ]
    }
   ],
   "source": [
    "# List of BBC News Regions\n",
    "region_urls = {\n",
    "    \"US & Canada\": \"https://www.bbc.com/news/us-canada\",\n",
    "    \"UK\": \"https://www.bbc.com/news/uk\",\n",
    "    \"Africa\": \"https://www.bbc.com/news/world/africa\",\n",
    "    \"Asia\": \"https://www.bbc.com/news/world/asia\",\n",
    "    \"Australia\": \"https://www.bbc.com/news/world/australia\",\n",
    "    \"Europe\": \"https://www.bbc.com/news/world/europe\",\n",
    "    \"Latin America\": \"https://www.bbc.com/news/world/latin_america\",\n",
    "    \"Middle East\": \"https://www.bbc.com/news/world/middle_east\",\n",
    "}\n",
    "\n",
    "# Initialize WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Storage for All Articles\n",
    "all_articles = []\n",
    "\n",
    "# Loop Through Each Region and Scrape Articles\n",
    "for region, url in region_urls.items():\n",
    "    print(f\"-- Scraping region: {region}\")\n",
    "    articles = extract_pages(url)  # Uses your existing function\n",
    "    all_articles.extend(articles)\n",
    "    print(f\" {len(articles)} articles scraped from {region}\")\n",
    "\n",
    "# ✅ Close the WebDriver\n",
    "driver.quit()\n",
    "\n",
    "# ✅ Print Final Results\n",
    "print(f\"\\nTotal articles scraped from all regions: {len(all_articles)}\")\n",
    "for article in all_articles[:10]:  # Display only first 10 articles\n",
    "    print(article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Saving results to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Articles saved to scraped_articles.csv ...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(all_articles)\n",
    "    \n",
    "    # Save the DataFrame to a CSV file\n",
    "df.to_csv(\"scraped_articles.csv\", index=False)  # index=False to avoid writing row indices\n",
    "    \n",
    "print(f\"... Articles saved to scraped_articles.csv ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Scraping Article Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading back the list of articles to use URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the scraped articles list\n",
    "file_path = \"scraped_articles.csv\"\n",
    "articles_df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Few articles have been inspected, four selectors for needed attributes have been found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Text Scraping Function. Using identifyed selectors the function gets the required attributes of an article from a given URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_article(driver, url):\n",
    "    \n",
    "    driver.get(url)\n",
    "    article_data = {\"url\": url, \"headline\": None, \"date\": None, \"author\": None, \"text\": None}\n",
    "    excnt = 0\n",
    "\n",
    "    try:\n",
    "        #print(f\"Extracting data from: {url}\")\n",
    "\n",
    "        # Extract Headline\n",
    "        try:\n",
    "            headline = WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, \"div[data-component='headline-block'] h1\"))\n",
    "            ).text\n",
    "            article_data[\"headline\"] = headline\n",
    "            #print(f\"Headline: {headline}\")\n",
    "        except:\n",
    "            excnt = excnt + 1\n",
    "            #print(\"Headline not found\")\n",
    "\n",
    "        # Extract Published Date\n",
    "        try:\n",
    "            date = driver.find_element(By.CSS_SELECTOR, \"time.sc-b42e7a8f-2\").text\n",
    "            article_data[\"date\"] = date\n",
    "            #print(f\"Published Date: {date}\")\n",
    "        except:\n",
    "            excnt = excnt + 1\n",
    "            #print(\"Date not found\")\n",
    "\n",
    "        # Extract Author\n",
    "        try:\n",
    "            author = driver.find_element(By.CSS_SELECTOR, \"span.sc-b42e7a8f-7\").text\n",
    "            article_data[\"author\"] = author\n",
    "            #print(f\"Author: {author}\")\n",
    "        except:\n",
    "            excnt = excnt + 1\n",
    "            #print(\"Author not found\")\n",
    "\n",
    "        # Extract Full Article Text\n",
    "        try:\n",
    "            paragraphs = driver.find_elements(By.CSS_SELECTOR, \"div[data-component='text-block'] p\")\n",
    "            text = \"\\n\".join([p.text for p in paragraphs])\n",
    "            article_data[\"text\"] = text\n",
    "            #print(f\"Extracted {len(paragraphs)} paragraphs\")\n",
    "        except:\n",
    "            excnt = excnt + 1\n",
    "            #print(\"Article text not found\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {e}\")\n",
    "\n",
    "    #print(f\"Total exceptions: {excnt}\")    \n",
    "\n",
    "    return article_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Scrape all articles. The code below uses the function defined above to iterate through URLs and get the required attributes. Error handling is implemented in the function itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting article scraping...\n",
      "\n",
      " Successfully scraped 707 articles.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Track scraped articles\n",
    "scraped_articles = []\n",
    "\n",
    "# Loop through first 5 articles (for debugging)\n",
    "print(\"Starting article scraping...\")\n",
    "for i, url in enumerate(articles_df[\"link\"], start=1):\n",
    "    #print(f\"\\n Scraping article {i}: {url}\")\n",
    "\n",
    "    article_data = scrape_article(driver, url)\n",
    "    scraped_articles.append(article_data)\n",
    "\n",
    "    # Print success message per article\n",
    "    #print(f\"Finished scraping article {i}\")\n",
    "\n",
    "# Close WebDriver\n",
    "driver.quit()\n",
    "\n",
    "# Convert to DataFrame\n",
    "scraped_articles_df = pd.DataFrame(scraped_articles)\n",
    "\n",
    "# Print the number of successfully scraped articles\n",
    "print(f\"\\n Successfully scraped {len(scraped_articles)} articles.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Saving the results to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped articles saved to: articles_full.csv\n"
     ]
    }
   ],
   "source": [
    "# Save the results to a CSV file with proper quoting\n",
    "output_file = \"articles_full.csv\"\n",
    "scraped_articles_df.to_csv(output_file, index=False, quoting=1)  # quoting=1 ensures double quotes for text fields\n",
    "\n",
    "print(f\"Scraped articles saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Discussion. Including the newly scraped data in the dataset is not straightforward because there are no labels. Without knowing whether the articles are \"fake\" or \"reliable,\" the data cannot be directly used for supervised learning. However, if labeled properly, it could enhance the dataset by increasing its size and diversity, potentially improving model generalization. The risk is that the new data might introduce bias or imbalance if it does not match the original distribution. A statistical comparison of word frequency, article length, and source credibility between the original and scraped datasets could help determine compatibility. If the differences are minimal, incorporating the new data—after labeling—could be beneficial. There is also an option to consider those news coming from BBC as reliable, then it could help with balancing the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final remark: there are many print() calls commented in the code to make pdf more compact."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
