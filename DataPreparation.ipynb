{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezfqps57qNtC"
      },
      "source": [
        "# Fake News Project - Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Vu1EICyV5Vy3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Error loading punkt: <urlopen error [Errno 8] nodename nor\n",
            "[nltk_data]     servname provided, or not known>\n",
            "[nltk_data] Error loading stopwords: <urlopen error [Errno 8] nodename\n",
            "[nltk_data]     nor servname provided, or not known>\n",
            "[nltk_data] Error loading punkt_tab: <urlopen error [Errno 8] nodename\n",
            "[nltk_data]     nor servname provided, or not known>\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# imports\n",
        "import pandas as pd\n",
        "import re\n",
        "import os\n",
        "import multiprocess as mp\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Downloading NLTK resources\n",
        "nltk.download(\"punkt\")  # Standard tokenizer\n",
        "nltk.download(\"stopwords\")  # Common stopwords\n",
        "nltk.download('punkt_tab')  # Specific requirement for your setup\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fiTaIONqrPD"
      },
      "source": [
        "## Part 1. Data Processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "APOgqPx76Sfe"
      },
      "outputs": [],
      "source": [
        "#Common functions definitions\n",
        "\n",
        "# Function to clean text\n",
        "def clean_text(text):\n",
        "    if pd.isna(text):\n",
        "        return text  # Keep NaN as is\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    text = re.sub(r'https?://\\S+', '<URL>', text)  # Replace URLs\n",
        "    text = re.sub(r'\\b\\d{1,4}[-/]\\d{1,2}[-/]\\d{1,4}\\b', '<DATE>', text)  # Replace Dates\n",
        "    text = re.sub(r'\\b\\d+\\b', '<NUM>', text)  # Replace Numbers\n",
        "    text = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', '<EMAIL>', text)  # Replace Emails\n",
        "    return text\n",
        "\n",
        "# Function to process text: Tokenization + Stopword Removal + Stemming\n",
        "def process_text(text):\n",
        "    if pd.isna(text):\n",
        "        return []\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word.lower() for word in tokens if word.isalpha() and word.lower() not in stop_words]\n",
        "    stemmed_tokens = [ps.stem(word) for word in tokens]\n",
        "    return stemmed_tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "vgWqBeCD6cw5"
      },
      "outputs": [],
      "source": [
        "# Initialization\n",
        "\n",
        "chunk_size = 50000  # Process 50,000 rows at a time\n",
        "num_cores = os.cpu_count()  # Use all available CPU cores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WprS29y5qyGa"
      },
      "source": [
        "Task 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "iR7wlP17qORy"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading CSV...\n",
            "CSV Loaded. Total rows in df: 250\n",
            "Index(['Unnamed', 'id', 'domain', 'type', 'url', 'content', 'scraped_at',\n",
            "       'inserted_at', 'updated_at', 'title', 'authors', 'keywords',\n",
            "       'meta_keywords', 'meta_description', 'tags', 'summary'],\n",
            "      dtype='object')\n",
            "Cleaning text...\n",
            "Text cleaning complete.\n",
            "Creating clean_df...\n",
            "Standardization complete.\n",
            "Applying filtering...\n",
            "Filtering complete. Total rows in clean_df: 232\n",
            "Saving clean_df to clean250.csv...\n",
            "File saved as clean250.csv\n"
          ]
        }
      ],
      "source": [
        "# Load CSV into df\n",
        "print(\"Loading CSV...\")\n",
        "df = pd.read_csv(\"news_sample.csv\")  # Ensure correct file name\n",
        "print(f\"CSV Loaded. Total rows in df: {len(df)}\")\n",
        "\n",
        "df.rename(columns={'Unnamed: 0': 'Unnamed'}, inplace=True)\n",
        "print(df.columns)\n",
        "\n",
        "# Parallel processing for text cleaning\n",
        "print(\"Cleaning text...\")\n",
        "with mp.Pool(num_cores) as pool:\n",
        "    cleaned_content = pool.map(clean_text, df[\"content\"])\n",
        "print(\"Text cleaning complete.\")\n",
        "\n",
        "# Create new DataFrame with cleaned data\n",
        "print(\"Creating clean_df...\")\n",
        "clean_df = df.copy()  # Preserve original df\n",
        "clean_df[\"content\"] = cleaned_content  # Assign cleaned text\n",
        "\n",
        "# Standardize other columns\n",
        "clean_df[\"label\"] = df[\"type\"].str.lower()\n",
        "clean_df[\"domain\"] = df[\"domain\"].str.lower()\n",
        "clean_df[\"title\"] = df[\"title\"].str.lower()\n",
        "clean_df[\"authors\"] = df[\"authors\"].str.lower()\n",
        "print(\"Standardization complete.\")\n",
        "\n",
        "# Apply filtering after cleaning\n",
        "print(\"Applying filtering...\")\n",
        "clean_df = clean_df[\n",
        "    (clean_df[\"label\"] != \"unknown\") &  # Remove 'unknown' labels\n",
        "    (clean_df[\"type\"].notna()) &  # Remove null labels\n",
        "    (clean_df[\"content\"].notna()) &  # Remove null content\n",
        "    (clean_df[\"content\"].str.len() > 10) &  # Remove very short articles\n",
        "    (clean_df[\"content\"].apply(lambda x: len(re.findall(r'\\b\\w+\\b', str(x))) > 1))  # Ensure multi-word articles\n",
        "]\n",
        "\n",
        "print(f\"Filtering complete. Total rows in clean_df: {len(clean_df)}\")\n",
        "\n",
        "# Save clean_df to CSV\n",
        "print(\"Saving clean_df to clean250.csv...\")\n",
        "clean_df.to_csv(\"clean250.csv\", index=False)\n",
        "print(\"File saved as clean250.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "98jPX4RCq_hy"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing clean250.csv in chunks of 50000 rows...\n",
            "Processing Chunk 1...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chunk 1 processed and saved!\n",
            "\n",
            " Processing complete! Tokenized & stemmed data saved to nltk250.csv\n"
          ]
        }
      ],
      "source": [
        "# Define input and output files\n",
        "input_file = \"clean250.csv\"\n",
        "output_file = \"nltk250.csv\"\n",
        "\n",
        "# Load stopwords\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "ps = PorterStemmer()  # Initialize stemmer\n",
        "\n",
        "\n",
        "\n",
        "# Open output file to write processed chunks\n",
        "print(f\"Processing {input_file} in chunks of {chunk_size} rows...\")\n",
        "\n",
        "for i, chunk in enumerate(pd.read_csv(input_file, chunksize=chunk_size)):\n",
        "    print(f\"Processing Chunk {i+1}...\")\n",
        "\n",
        "    # Parallel processing\n",
        "    with mp.Pool(num_cores) as pool:\n",
        "        chunk[\"tokens\"] = pool.map(process_text, chunk[\"content\"])\n",
        "\n",
        "    # Drop original content column to save space\n",
        "    chunk.drop(columns=[\"content\"], inplace=True)\n",
        "\n",
        "    # Write the processed chunk to the output file (append after first write)\n",
        "    mode = \"w\" if i == 0 else \"a\"\n",
        "    header = (i == 0)  # Write header only for the first chunk\n",
        "    chunk.to_csv(output_file, index=False, mode=mode, header=header)\n",
        "\n",
        "    print(f\"Chunk {i+1} processed and saved!\")\n",
        "\n",
        "print(f\"\\n Processing complete! Tokenized & stemmed data saved to {output_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpTRX5-ssjdY"
      },
      "source": [
        "Task 3. Located before Task 2 because original and cleaned datasets should be compared."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "7Jyvgr8dsV5w"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading CSV...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/jy/c7h9f73j73v5pv4bpl0m77zh0000gp/T/ipykernel_5686/352275029.py:3: DtypeWarning: Columns (0,1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(\"995,000_rows.csv\")  # Ensure correct file name\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CSV Loaded. Total rows in df: 995000\n",
            "Index(['Unnamed', 'id', 'domain', 'type', 'url', 'content', 'scraped_at',\n",
            "       'inserted_at', 'updated_at', 'title', 'authors', 'keywords',\n",
            "       'meta_keywords', 'meta_description', 'tags', 'summary', 'source'],\n",
            "      dtype='object')\n",
            "Cleaning text in parallel...\n",
            "Text cleaning complete.\n",
            "Creating clean_df...\n",
            "Standardization complete.\n",
            "Applying filtering...\n",
            "Filtering complete. Total rows in clean_df: 903679\n",
            "Saving clean_df to 995k-cleaned.csv...\n",
            "File saved as 995k-cleaned.csv\n"
          ]
        }
      ],
      "source": [
        "# Load CSV into df\n",
        "print(\"Loading CSV...\")\n",
        "df = pd.read_csv(\"995,000_rows.csv\")  # Ensure correct file name\n",
        "print(f\"CSV Loaded. Total rows in df: {len(df)}\")\n",
        "\n",
        "df.rename(columns={'Unnamed: 0': 'Unnamed'}, inplace=True)\n",
        "print(df.columns)\n",
        "\n",
        "# Parallel processing for text cleaning\n",
        "print(\"Cleaning text in parallel...\")\n",
        "with mp.Pool(num_cores) as pool:\n",
        "    cleaned_content = pool.map(clean_text, df[\"content\"])\n",
        "print(\"Text cleaning complete.\")\n",
        "\n",
        "# Create new DataFrame with cleaned data\n",
        "print(\"Creating clean_df...\")\n",
        "clean_df = df.copy()  # Preserve original df\n",
        "clean_df[\"content\"] = cleaned_content  # Assign cleaned text\n",
        "\n",
        "# Standardize other columns\n",
        "clean_df[\"label\"] = df[\"type\"].str.lower()\n",
        "clean_df[\"domain\"] = df[\"domain\"].str.lower()\n",
        "clean_df[\"title\"] = df[\"title\"].str.lower()\n",
        "clean_df[\"authors\"] = df[\"authors\"].str.lower()\n",
        "print(\"Standardization complete.\")\n",
        "\n",
        "# Apply filtering after cleaning\n",
        "print(\"Applying filtering...\")\n",
        "clean_df = clean_df[\n",
        "    (clean_df[\"label\"] != \"unknown\") &  # Remove 'unknown' labels\n",
        "    (clean_df[\"type\"].notna()) &  # Remove null labels\n",
        "    (clean_df[\"content\"].notna()) &  # Remove null content\n",
        "    (clean_df[\"content\"].str.len() > 10) &  # Remove very short articles\n",
        "    (clean_df[\"content\"].apply(lambda x: len(re.findall(r'\\b\\w+\\b', str(x))) > 1))  # Ensure multi-word articles\n",
        "]\n",
        "\n",
        "print(f\"Filtering complete. Total rows in clean_df: {len(clean_df)}\")\n",
        "\n",
        "# Save clean_df to CSV\n",
        "print(\"Saving clean_df to 995k-cleaned.csv...\")\n",
        "clean_df.to_csv(\"995k-cleaned.csv\", index=False)\n",
        "print(\"File saved as 995k-cleaned.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "hyfj15w_suAI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 995k-cleaned.csv in batches of 50000 rows...\n",
            " Chunk 1\n",
            "Chunk 1 saved.\n",
            " Chunk 2\n",
            "Chunk 2 saved.\n",
            " Chunk 3\n",
            "Chunk 3 saved.\n",
            " Chunk 4\n",
            "Chunk 4 saved.\n",
            " Chunk 5\n",
            "Chunk 5 saved.\n",
            " Chunk 6\n",
            "Chunk 6 saved.\n",
            " Chunk 7\n",
            "Chunk 7 saved.\n",
            " Chunk 8\n",
            "Chunk 8 saved.\n",
            " Chunk 9\n",
            "Chunk 9 saved.\n",
            " Chunk 10\n",
            "Chunk 10 saved.\n",
            " Chunk 11\n",
            "Chunk 11 saved.\n",
            " Chunk 12\n",
            "Chunk 12 saved.\n",
            " Chunk 13\n",
            "Chunk 13 saved.\n",
            " Chunk 14\n",
            "Chunk 14 saved.\n",
            " Chunk 15\n",
            "Chunk 15 saved.\n",
            " Chunk 16\n",
            "Chunk 16 saved.\n",
            " Chunk 17\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/jy/c7h9f73j73v5pv4bpl0m77zh0000gp/T/ipykernel_5686/259855899.py:33: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  for i, chunk in enumerate(reader):\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chunk 17 saved.\n",
            " Chunk 18\n",
            "Chunk 18 saved.\n",
            " Chunk 19\n",
            "Chunk 19 saved.\n",
            "\n",
            " All done! Final preprocessed file saved as 995pre.csv\n"
          ]
        }
      ],
      "source": [
        "# Input/output settings\n",
        "input_file = \"995k-cleaned.csv\"\n",
        "output_file = \"995pre.csv\"\n",
        "chunk_size = 50000\n",
        "num_cores = os.cpu_count()\n",
        "\n",
        "# Load stopwords and stemmer\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "ps = PorterStemmer()\n",
        "\n",
        "# Label mapping\n",
        "fake_labels = [\n",
        "    \"fake\", \"conspiracy\", \"bias\", \"unreliable\", \"rumor\",\n",
        "    \"clickbait\", \"junksci\", \"hate\", \"satire\", \"political\"\n",
        "]\n",
        "real_labels = [\"reliable\"]\n",
        "\n",
        "def map_label(label):\n",
        "    label = str(label).lower()\n",
        "    if label in fake_labels:\n",
        "        return 0\n",
        "    elif label in real_labels:\n",
        "        return 1\n",
        "    return None\n",
        "\n",
        "# Start batch processing\n",
        "if os.path.exists(output_file):\n",
        "    os.remove(output_file)  # Clear old output if exists\n",
        "\n",
        "print(f\"Processing {input_file} in batches of {chunk_size} rows...\")\n",
        "\n",
        "reader = pd.read_csv(input_file, chunksize=chunk_size)\n",
        "for i, chunk in enumerate(reader):\n",
        "    print(f\" Chunk {i+1}\")\n",
        "\n",
        "    # Clean labels â†’ binary\n",
        "    chunk[\"binary_label\"] = chunk[\"label\"].apply(map_label)\n",
        "    chunk = chunk[chunk[\"binary_label\"].notna()].copy()\n",
        "    chunk[\"binary_label\"] = chunk[\"binary_label\"].astype(int)\n",
        "\n",
        "    # Tokenize + stem in parallel\n",
        "    with mp.Pool(num_cores) as pool:\n",
        "        chunk[\"tokens\"] = pool.map(process_text, chunk[\"content\"])\n",
        "\n",
        "    # Create 'text' column\n",
        "    chunk[\"text\"] = chunk[\"tokens\"].apply(lambda x: \" \".join(x))\n",
        "\n",
        "    # Keep only required + useful columns\n",
        "    keep_cols = [\"tokens\", \"text\", \"binary_label\"] + [col for col in [\"title\", \"domain\", \"authors\", \"source\", \"type\", \"label\"] if col in chunk.columns]\n",
        "    chunk_out = chunk[keep_cols]\n",
        "\n",
        "    # Write to output\n",
        "    mode = \"w\" if i == 0 else \"a\"\n",
        "    header = (i == 0)\n",
        "    chunk_out.to_csv(output_file, index=False, mode=mode, header=header)\n",
        "\n",
        "    print(f\"Chunk {i+1} saved.\")\n",
        "\n",
        "print(f\"\\n All done! Final preprocessed file saved as {output_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocessing BBC dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 707 rows from articles_full.csv\n",
            "Cleaning text...\n",
            "Processing tokens...\n",
            "Saved preprocessed file to articles_full_pre.csv\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Setup\n",
        "input_file = \"articles_full.csv\"\n",
        "output_file = \"articles_full_pre.csv\"\n",
        "\n",
        "# Load stopwords\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "ps = PorterStemmer()  # Initialize stemmer\n",
        "\n",
        "\n",
        "# Load file\n",
        "df = pd.read_csv(input_file)\n",
        "print(f\"Loaded {len(df)} rows from {input_file}\")\n",
        "\n",
        "# Clean 'text' using existing clean_text()\n",
        "print(\"Cleaning text...\")\n",
        "with mp.Pool(num_cores) as pool:\n",
        "    df[\"text\"] = pool.map(clean_text, df[\"text\"])\n",
        "\n",
        "# Tokenize and stem using existing process_text()\n",
        "print(\"Processing tokens...\")\n",
        "with mp.Pool(num_cores) as pool:\n",
        "    df[\"tokens\"] = pool.map(process_text, df[\"text\"])\n",
        "\n",
        "# Reconstruct final 'text' and add label\n",
        "df[\"text\"] = df[\"tokens\"].apply(lambda x: \" \".join(x))\n",
        "df[\"binary_label\"] = 1  # All articles are considered reliable\n",
        "\n",
        "# Keep only what we need\n",
        "df_out = df[[\"tokens\", \"text\", \"binary_label\"]]\n",
        "\n",
        "# Save output\n",
        "df_out.to_csv(output_file, index=False)\n",
        "print(f\"Saved preprocessed file to {output_file}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
